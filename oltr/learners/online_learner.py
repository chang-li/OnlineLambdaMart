import numpy as np
import lightgbm as gbm

from oltr.utils.metric import ndcg_at_k
from oltr.utils.queries import Queries, find_constant_features


class OnlineLTR(object):

  def __init__(self, train_qset, valid_qset=None, test_qset=None, seed=42):

    self.seed = seed
    np.random.seed(seed)

    self.train_qset = train_qset
    cls = find_constant_features(self.train_qset)
    self.train_qset.adjust(remove_features=cls, purge=True, scale=True)

    self.valid_qset = valid_qset
    cls = find_constant_features(self.valid_qset)
    self.valid_qset.adjust(remove_features=cls, purge=True, scale=True)

    self.test_qset = test_qset
    cls = find_constant_features(self.test_qset)
    self.test_qset.adjust(remove_features=cls, purge=True, scale=True)

    # The previous collected training date.
    self.observed_training_data = []

  def sample_query_ids(self, num_queries, data='train'):
    qset = self.train_qset
    if data == 'valid':
      qset = self.valid_qset
    if data == 'test':
      qset = self.test_qset
    return np.random.choice(qset.n_queries, num_queries)

  def get_labels_and_rankings(self, ranker, num_queries):
    """Apply a ranker to a subsample of the data and get the labels and ranks.

    Args:
      ranker: A LightGBM model.
      num_queries: Number of queries to be sampled from self.train_qset

    Returns:
      A tuple of lists that assign labels and rankings to the documents of each
      query.
    """
    query_ids = self.sample_query_ids(num_queries)
    n_docs_per_query = [self.train_qset[qid].document_count() for qid in query_ids]
    indices = [0] + np.cumsum(n_docs_per_query).tolist()
    labels = [self.train_qset[qid].relevance_scores for qid in query_ids]

    # Get the rankings of document per query
    if ranker is None:
      rankings = [np.random.permutation(n_docs) for n_docs in n_docs_per_query]
    else:
      features = self.train_qset[query_ids].feature_vectors
      scores = ranker.predict(features)
      tie_breakers = np.random.rand(scores.shape[0])
      rankings = [np.lexsort((tie_breakers[indices[i]:indices[i+1]],
                              -scores[indices[i]:indices[i+1]]))
                  for i in range(num_queries)]

    return query_ids, labels, rankings

  def apply_click_model_to_labels_and_scores(self, click_model, labels,
                                             rankings):
    """This method samples some queries and generates clicks for them based on
    a click model.

    Args:
      click_model: a click model
      labels: true labels of documents
      rankings: ranking of documents of each query

    Returns:
      A list of clicks of documents of each query
    """
    clicks = [click_model.get_click(labels[i][rankings[i]])
              for i in range(len(rankings))]
    return clicks

  def generate_training_data_from_clicks(self, query_ids, clicks, rankings):
    """This method uses the clicks generated by
    apply_click_model_to_labels_and_scores to create a training dataset.

    Args:
      query_ids: the sampled query ids
      clicks: clicks from the click model

    Returns:
      A tuple of (train_features, train_labels):
        train_features: list of observed docs per query
        train_labels: list of click feedback per query
    """
    # last observed position of each ranking
    last_pos = []
    train_labels = []
    for click in clicks:
      if sum(click) == 0:
        last_pos.append(len(click))
      else:
        last_pos.append(np.where(click)[0][-1]+1)
      train_labels.append(click[:last_pos[-1]])

    train_indices = [self.train_qset.query_indptr[qid] + rankings[i][:last_pos[i]]
                     for i, qid in enumerate(query_ids)]
    
    # Cf. the following for an example:
    # https://mlexplained.com/2019/05/27/learning-to-rank-explained-with-code/
    
    # train_features = [self.train_qset.feature_vectors[idx] for idx in train_indices]
    # train_features = np.concatenate(train_features)
    # train_query_group = [feature.shape[0] for feature in train_features]

    train_query_group = np.asarray(last_pos)
    train_labels = np.concatenate(train_labels)
    self.observed_training_data.append((train_indices, train_labels,
                                        train_query_group))

  def update_ranker(self, ranker_params, fit_params):
    """"This method uses the training data from
    generate_training_data_from_clicks to improve the ranker."""
    if self.observed_training_data:
      train_indices = [inds for otd in self.observed_training_data
                       for inds in otd[0]]
      train_features = np.concatenate([self.train_qset.feature_vectors[inds]
                                       for inds in train_indices])
      train_labels = np.concatenate([otd[1]
                                     for otd in self.observed_training_data])
      train_query_group = np.concatenate([otd[2]
                                       for otd in self.observed_training_data])
    else:
      raise ValueError('OnlineLTR.generate_training_data_from_clicks()'
        'should be called before OnlineLTR.update_ranker().')

    ranker = gbm.LGBMRanker(**ranker_params)
    if 'early_stopping_rounds' in fit_params:
      num_queries = len(self.observed_training_data[-1][0])
      valid_query_ids = self.sample_query_ids(num_queries, data='valid')
      valid_labels = np.concatenate([self.valid_qset[qid].relevance_scores
                                     for qid in valid_query_ids])
      valid_features = self.valid_qset[valid_query_ids].feature_vectors
      valid_query_group = [self.valid_qset[qid].document_count() for qid in valid_query_ids]
      ranker.fit(X=train_features, y=train_labels, group=train_query_group,
                 eval_set=[(valid_features, valid_labels)], eval_group=[valid_query_group],
                 **fit_params)
    else:
      ranker.fit(X=train_features, y=train_labels, group=train_query_group,
                 **fit_params)
    return ranker

  def update_learner(self, ranker, num_train_queries, click_model,
                     ranker_params, fit_params):
    # Collect feedback
    train_query_ids, train_labels, train_rankings = \
      self.get_labels_and_rankings(ranker, num_train_queries)
    train_clicks = self.apply_click_model_to_labels_and_scores(
      click_model, train_labels, train_rankings)
    self.generate_training_data_from_clicks(
      train_query_ids, train_clicks, train_rankings)

    # Return retrained ranker
    return self.update_ranker(ranker_params, fit_params)

  def evaluate_ranker(self, ranker, eval_params, query_ids=None, data='test'):
    """ Evaluate the ranker based on the queries in self.train_qset
    :param ranker:
    :param eval_params:  ndcg, cutoff
    :return:
    """
    qset = self.test_qset
    if data == 'train':
      qset = self.train_qset
    if data == 'valid':
      qset = self.valid_qset
    if query_ids is None:
      eval_qset = qset
    else:
      eval_qset = qset[query_ids]
    scores = ranker.predict(eval_qset.feature_vectors)
    tie_breakers = np.random.rand(scores.shape[0])

    indices = eval_qset.query_indptr
    rankings = [np.lexsort((tie_breakers[indices[i]:indices[i + 1]],
                -scores[indices[i]:indices[i + 1]]))
                for i in range(eval_qset.n_queries)]
    # raise ValueError
    ndcgs = [eval_params['metric'](
                eval_qset[qid].relevance_scores[rankings[qid]],
                eval_params['cutoff'])
             for qid in range(eval_qset.n_queries)]
    return np.mean(ndcgs)

